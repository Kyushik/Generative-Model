{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGGAN CelebA\n",
    "\n",
    "This notebook is for implementing `Progressive Growing Generative Adversarial Network(PGGAN)` from the paper [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/abs/1710.10196) with [Tensorflow](https://www.tensorflow.org). <br>\n",
    "[CelebA dataset](https://www.kaggle.com/jessicali9530/celeba-dataset), which is 128x128 size, will be used. \n",
    "\n",
    "Reference: [hwalsuklee's Github](https://github.com/hwalsuklee/tensorflow-generative-model-collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmc-server1/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mmc-server1/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mmc-server1/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mmc-server1/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mmc-server1/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mmc-server1/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob\n",
    "import cv2 \n",
    "import datetime\n",
    "import os\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'PGGAN_CelebA'\n",
    "\n",
    "img_size = 128\n",
    "\n",
    "n_latent = 512\n",
    "\n",
    "beta1 = 0\n",
    "beta2 = 0.9\n",
    "\n",
    "show_result_step = 1000\n",
    "\n",
    "minibatch_repeat = 1\n",
    "\n",
    "batch_list = [128, 128, 128, 64, 32, 16]\n",
    "epoch_list = [2, 2, 5, 18, 34, 66]\n",
    "size_list = [[4], [8], [8, 10, 12, 14, 16], \n",
    "             list(range(16, 33, 2)), \n",
    "             list(range(32, 65, 2)), \n",
    "             list(range(64, 129, 2))]\n",
    "\n",
    "channel_list_g = [64, 128, 256, 512, 512, 512]\n",
    "channel_list_d = [64, 128, 256, 512, 512, 512]\n",
    "\n",
    "lr_list_g = [2e-4, 2e-4, 2e-4, 1e-4, 1e-4, 1e-4]\n",
    "lr_list_d = [2e-4, 2e-4, 2e-4, 1e-4, 1e-4, 1e-4]\n",
    "\n",
    "lr_g_decay_rate = 1.0\n",
    "lr_d_decay_rate = 1.0\n",
    "\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d-%H-%M-%S\")\n",
    "\n",
    "start_level = 0\n",
    "\n",
    "load_model = False\n",
    "train_model = True\n",
    "\n",
    "save_path = \"./saved_models/\" + date_time + \"_\" + algorithm\n",
    "#load_path = \"./saved_models/20191106-22-06-58_PGGAN_CelebA/16/model\" \n",
    "# load_path = \"./saved_models/20191108-13-05-01_PGGAN_CelebA/16/model\" \n",
    "load_path = \"./saved_models/20191113-11-44-32_PGGAN_CelebA/16/model\" \n",
    "\n",
    "# WGAN_GP Parateter\n",
    "n_critic = 1\n",
    "d_lambda = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CelebA Dataset\n",
    "\n",
    "Get names of the files in the celeba dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CelebA dataset Length: 202599\n"
     ]
    }
   ],
   "source": [
    "celebA_list = glob.glob('./img_align_celeba/*.jpg')\n",
    "\n",
    "print(\"CelebA dataset Length: {}\".format(len(celebA_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGGAN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGGAN():\n",
    "    def __init__(self, stage):\n",
    "        self.stage = stage\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='x_'+str(stage))\n",
    "        self.x_normalize = (tf.cast(self.x, tf.float32) - (1.0/2)) / (1.0/2)\n",
    "\n",
    "        self.z = tf.placeholder(tf.float32, shape=[None, n_latent], name='z_'+str(stage))\n",
    "        \n",
    "        self.alpha = tf.placeholder(tf.float32, shape=[1])\n",
    "        self.batch_size = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.lr_g = tf.placeholder(tf.float32)\n",
    "        self.lr_d = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.size_list = tf.placeholder(tf.float32, shape=[None])\n",
    "        \n",
    "        self.d_loss, self.g_loss, self.G = self.GAN(self.x_normalize, self.z) \n",
    "\n",
    "        # optimization\n",
    "        self.trainable_vars = tf.trainable_variables()\n",
    "\n",
    "        self.trainable_vars_d = [var for var in self.trainable_vars if var.name.startswith('Discriminator' + str(self.stage))]\n",
    "        self.trainable_vars_g = [var for var in self.trainable_vars if var.name.startswith('Generator' + str(self.stage))]\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer_d = tf.train.AdamOptimizer(self.lr_d, beta1, beta2)\n",
    "            gvs_d = optimizer_d.compute_gradients(self.d_loss, var_list=self.trainable_vars_d)\n",
    "            capped_gvs_d = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs_d]\n",
    "            self.train_step_d = optimizer_d.apply_gradients(capped_gvs_d)\n",
    "            \n",
    "            optimizer_g = tf.train.AdamOptimizer(self.lr_g, beta1, beta2)\n",
    "            gvs_g = optimizer_g.compute_gradients(self.g_loss, var_list=self.trainable_vars_g)\n",
    "            capped_gvs_g = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs_g]\n",
    "            self.train_step_g = optimizer_g.apply_gradients(capped_gvs_g)\n",
    "\n",
    "    def GAN(self, x, z):\n",
    "        # Generator\n",
    "        G, mini_std = self.Generator(z)\n",
    "\n",
    "        # Discriminator\n",
    "        D_logit_real, D_out_real = self.Discriminator(x, mini_std)\n",
    "        D_logit_fake, D_out_fake = self.Discriminator(G, mini_std, reuse=True)\n",
    "    \n",
    "        # get loss (LSGAN)\n",
    "        ########################################### LSGAN ###########################################\n",
    "        d_loss = tf.reduce_mean(tf.square(D_logit_real-1)) + tf.reduce_mean(tf.square(D_logit_fake)) + 0.001*tf.reduce_mean(tf.square(D_logit_real))\n",
    "        g_loss = tf.reduce_mean(tf.square(D_logit_fake-1))\n",
    "        #############################################################################################\n",
    "        \n",
    "        return d_loss, g_loss , G\n",
    "    \n",
    "    # Pixel Normalization\n",
    "    def pixel_norm(self, x, epsilon=1e-8):\n",
    "        return x * tf.rsqrt(tf.reduce_mean(tf.square(x), axis=-1, keepdims=True) + epsilon)\n",
    "    \n",
    "    # Minibatch Standard Deviation\n",
    "    def minibatch_std(self, x):\n",
    "        # Compute Standard Deviation over minibatch\n",
    "        _, batch_std = tf.nn.moments(x, axes=[0], keep_dims=True)\n",
    "        \n",
    "        # Average all features\n",
    "        feature_avg = tf.reduce_mean(batch_std, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Replicate the value to concatenate it over the minibatch\n",
    "        out = tf.tile(feature_avg, multiples=[self.batch_size, 1, 1, 1])\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def conv_block_g(self, x, num_channel, network_name):        \n",
    "        h1 = tf.layers.conv2d(x, filters=num_channel, kernel_size=3, strides=1, padding='SAME', name=network_name+'_1')\n",
    "        h1 = tf.nn.leaky_relu(self.pixel_norm(h1))  \n",
    "            \n",
    "        h2 = tf.layers.conv2d(h1, filters=num_channel, kernel_size=3, strides=1, padding='SAME', name=network_name+'_2')\n",
    "        h2 = tf.nn.leaky_relu(self.pixel_norm(h2))  \n",
    "        \n",
    "        return h2\n",
    "\n",
    "    \n",
    "    def conv_block_d(self, x, num_channel, network_name):\n",
    "        h1 = tf.layers.conv2d(x, filters=num_channel, kernel_size=3, strides=1, padding='SAME', name=network_name+'_1')\n",
    "        h1 = tf.nn.leaky_relu(h1)\n",
    "        \n",
    "        h2 = tf.layers.conv2d(h1, filters=num_channel, kernel_size=3, strides=1, padding='SAME', name=network_name+'_2')\n",
    "        h2 = tf.nn.leaky_relu(h2)\n",
    "\n",
    "        return h2\n",
    "\n",
    "    def Generator(self, x, reuse=False):\n",
    "        with tf.variable_scope('Generator' + str(self.stage), reuse=reuse):       \n",
    "            # Project and Reshape \n",
    "            z_size = int(size_list[0][0]/4)\n",
    "            \n",
    "            x_project = tf.layers.dense(x, z_size*z_size*n_latent)\n",
    "            x_reshape = tf.reshape(x_project, (-1, z_size, z_size, n_latent))\n",
    "            \n",
    "            h1 = tf.layers.conv2d_transpose(x_reshape,filters=channel_list_g[-1], kernel_size=4, strides=4, padding='SAME')\n",
    "            h1 = tf.nn.leaky_relu(self.pixel_norm(h1))\n",
    "            \n",
    "            h2 = tf.layers.conv2d(h1,filters=channel_list_g[-1], kernel_size=3, strides=1, padding='SAME')\n",
    "            h2 = tf.nn.leaky_relu(self.pixel_norm(h2))  \n",
    "\n",
    "            mini_std = self.minibatch_std(h2)\n",
    "        \n",
    "            in_block = h2\n",
    "            out_block = h2\n",
    "            upsample = h2\n",
    "\n",
    "            for i in range(self.stage):\n",
    "                upsample = tf.image.resize_nearest_neighbor(in_block, (self.size_list[i], self.size_list[i]))\n",
    "                out_block = self.conv_block_g(upsample, channel_list_g[-2-i], 'block'+str(i))\n",
    "                in_block = out_block\n",
    "\n",
    "            # Output layer           \n",
    "            RGB1 = tf.layers.conv2d(upsample, filters=3, kernel_size=1, strides=1, padding='SAME', name='RGB1_'+str(self.stage)) \n",
    "            RGB2 = tf.layers.conv2d(out_block, filters=3, kernel_size=1, strides=1, padding='SAME', name='RGB2_'+str(self.stage)) \n",
    "            \n",
    "            img = (1-self.alpha)*RGB1 + self.alpha*RGB2\n",
    "            \n",
    "            if self.stage != 0:\n",
    "                img = tf.image.resize_nearest_neighbor(img, (self.size_list[-1], self.size_list[-1]))\n",
    "                img = tf.layers.conv2d(img, filters=3, kernel_size=1, strides=1, padding='SAME', name='RGB_'+str(self.stage)) \n",
    "            \n",
    "            output = tf.tanh(img)\n",
    "        \n",
    "        return output, mini_std      \n",
    "\n",
    "\n",
    "    def Discriminator(self, x, mini_std, reuse=False):\n",
    "        with tf.variable_scope('Discriminator' + str(self.stage), reuse=reuse):\n",
    "            \n",
    "            h1 = tf.layers.conv2d(x, filters=channel_list_d[-self.stage-1], kernel_size=1, strides=1, activation=tf.nn.leaky_relu, \n",
    "                      padding='SAME', name='h1_'+str(self.stage))\n",
    "                \n",
    "            in_block = h1\n",
    "            out_block = h1\n",
    "            \n",
    "            for i in range(self.stage):\n",
    "                out_block = self.conv_block_d(in_block, channel_list_d[-self.stage+i], 'block'+str(self.stage-1-i))\n",
    "                \n",
    "                if i == 0:\n",
    "                    out_downsample = tf.layers.average_pooling2d(out_block, 2, 2)\n",
    "                    in_downsample = tf.layers.average_pooling2d(in_block, 2, 2)\n",
    "                    in_feature = tf.layers.conv2d(in_downsample, filters=out_downsample.get_shape()[3], \n",
    "                                                  kernel_size=1, strides=1, padding='SAME', name='feature_'+str(self.stage))\n",
    "                    out_block = (1-self.alpha)*in_feature + self.alpha*out_downsample\n",
    "                else:\n",
    "                    out_block = tf.layers.average_pooling2d(out_block, 2, 2)\n",
    "                    \n",
    "                in_block = out_block           \n",
    "            \n",
    "            # Output layer\n",
    "#             out_add_std = tf.concat([out_block, mini_std], axis=-1)\n",
    "            \n",
    "            h2 = tf.layers.conv2d(out_block, filters=channel_list_d[-1], kernel_size=3, strides=1, \n",
    "                                  activation=tf.nn.leaky_relu, padding='SAME', name='h2')\n",
    "            h3 = tf.layers.conv2d(h2, filters=channel_list_d[-1], kernel_size=4, strides=4, \n",
    "                                  activation=tf.nn.leaky_relu, padding='SAME', name='h3')\n",
    "            \n",
    "            # Output layer\n",
    "            flatten = tf.reshape(h3, (-1, h3.get_shape()[3]))\n",
    "\n",
    "            logit  = tf.layers.dense(flatten, 1, name='logit_'+str(self.stage))\n",
    "            output = tf.sigmoid(logit)  \n",
    "\n",
    "        return logit, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "\n",
    "for i in range(len(size_list)):\n",
    "    model_list.append(PGGAN(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Saver = tf.train.Saver()\n",
    "\n",
    "if load_model == True:\n",
    "    Saver.restore(sess, load_path)\n",
    "    \n",
    "    if start_level != 0:\n",
    "        model_old = model_list[start_level-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / Batch: 0.632% / G Loss: 0.46165 / D Loss: 0.43789 / LR D: 0.00020 / LR G: 0.00020 / Img Size: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 / Batch: 0.263% / G Loss: 0.25585 / D Loss: 0.49206 / LR D: 0.00020 / LR G: 0.00020 / Img Size: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1140 / 1582, Alpha: 0.9601202375135118\r"
     ]
    }
   ],
   "source": [
    "if train_model:\n",
    "    # Training\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "    len_data = len(celebA_list)\n",
    "\n",
    "    for m in range(start_level, len(size_list)):\n",
    "        # Define models and set parameters for new level\n",
    "        model = model_list[m]\n",
    "        alpha = 0\n",
    "        count_step = 0\n",
    "        \n",
    "        batch_size = batch_list[m]\n",
    "        epoch = epoch_list[m]\n",
    "        \n",
    "        size_up_epoch = int(epoch/len(size_list[m]))\n",
    "        \n",
    "        learning_rate_g = lr_list_g[m]\n",
    "        learning_rate_d = lr_list_d[m]\n",
    "        \n",
    "        # Assign the parameters to the new layers\n",
    "        if m != 0:\n",
    "            model_var_d = model.trainable_vars_d\n",
    "            model_var_g = model.trainable_vars_g\n",
    "            \n",
    "            model_old_var_d = model_old.trainable_vars_d\n",
    "            model_old_var_g = model_old.trainable_vars_g\n",
    "            \n",
    "            for var_d in model_var_d:\n",
    "                var_d_split = var_d.name.split('/')\n",
    "                for var_old_d in model_old_var_d:\n",
    "                    var_old_d_split = var_old_d.name.split('/')\n",
    "                    \n",
    "                    if var_d_split[1] == var_old_d_split[1] and var_d_split[2] == var_old_d_split[2]:\n",
    "                        sess.run(var_d.assign(var_old_d))\n",
    "\n",
    "            for var_g in model_var_g:\n",
    "                var_g_split = var_g.name.split('/')\n",
    "                for var_old_g in model_old_var_g:\n",
    "                    var_old_g_split = var_old_g.name.split('/')\n",
    "                    \n",
    "                    if var_g_split[1] == var_old_g_split[1] and var_g_split[2] == var_old_g_split[2]:\n",
    "                        sess.run(var_g.assign(var_old_g))\n",
    "            \n",
    "            print('------------------- Var Assign is Finished! -------------------')\n",
    "        for i in range(epoch):\n",
    "            # Shuffle the data \n",
    "            np.random.shuffle(celebA_list)\n",
    "            \n",
    "            count_batch = 0\n",
    "            \n",
    "            # Making mini-batch\n",
    "            for j in range(0, len_data, batch_size):\n",
    "                # Get alpha for weighting the new layers \n",
    "                alpha = 1.2 * i/size_up_epoch + (1/size_up_epoch) * (j / len_data)\n",
    "                \n",
    "                if alpha > 1:\n",
    "                    alpha = 1.0\n",
    "                \n",
    "                if j + batch_size < len_data:\n",
    "                    x_in = np.zeros([batch_size, img_size, img_size, 3])\n",
    "\n",
    "                    for k in range(batch_size):\n",
    "                        img_temp = cv2.imread(celebA_list[j + k])\n",
    "                        x_in[k,:,:,:] = img_temp[45:45 + img_size, 25: 25+img_size, :]\n",
    "\n",
    "                x_in = x_in.reshape((-1, img_size, img_size, 3))\n",
    "                \n",
    "                size_up_idx = int(i/size_up_epoch)\n",
    "                \n",
    "                m_size_list = []\n",
    "                for k in range(m):\n",
    "                    m_size_list.append(size_list[k][-1])\n",
    "                m_size_list.append(size_list[m][size_up_idx])\n",
    "                \n",
    "                x_in_resize = np.zeros([x_in.shape[0], size_list[m][size_up_idx], size_list[m][size_up_idx], 3])\n",
    "                \n",
    "                for k in range(x_in.shape[0]):\n",
    "                    x_in_resize[k,:,:,:] = resize(x_in[k,:,:,:], (size_list[m][size_up_idx], \n",
    "                                                                  size_list[m][size_up_idx]))\n",
    "                \n",
    "                x_in_resize = np.float32(x_in_resize / 255.0)\n",
    "                \n",
    "                sampled_z = np.random.uniform(-1, 1, size=(x_in_resize.shape[0] , n_latent))              \n",
    "                \n",
    "                for k in range(minibatch_repeat):\n",
    "                    # Run Optimizer!\n",
    "                    _, loss_d = sess.run([model.train_step_d, model.d_loss], feed_dict = {model.x: x_in_resize, \n",
    "                                                                                          model.z: sampled_z,\n",
    "                                                                                          model.alpha: [alpha],\n",
    "                                                                                          model.batch_size: batch_size,\n",
    "                                                                                          model.lr_d: learning_rate_d,\n",
    "                                                                                          model.lr_g: learning_rate_g,\n",
    "                                                                                          model.size_list: m_size_list})\n",
    "\n",
    "                    _, loss_g = sess.run([model.train_step_g, model.g_loss], feed_dict = {model.x: x_in_resize, \n",
    "                                                                                          model.z: sampled_z, \n",
    "                                                                                          model.alpha: [alpha],\n",
    "                                                                                          model.batch_size: batch_size,\n",
    "                                                                                          model.lr_d: learning_rate_d,\n",
    "                                                                                          model.lr_g: learning_rate_g,\n",
    "                                                                                          model.size_list: m_size_list})\n",
    "\n",
    "                print(\"Batch: {} / {}, Alpha: {}\".format(count_batch, int(len_data/batch_size), alpha), end=\"\\r\")\n",
    "                \n",
    "                count_batch += 1\n",
    "                \n",
    "                if count_step % show_result_step == 0 and count_step != 0:\n",
    "                    # Print Progess\n",
    "                    print(\"Epoch: {} / Batch: {:.3f}% / G Loss: {:.5f} / D Loss: {:.5f} / LR D: {:.5f} / LR G: {:.5f} / Img Size: {}\".format(\n",
    "                        (i+1), (j/len_data), loss_g, loss_d, learning_rate_d, learning_rate_g, size_list[m][size_up_idx]))\n",
    "\n",
    "                    # Show test images \n",
    "                    z_test = np.random.uniform(-1, 1, size=(5, n_latent))\n",
    "                    G_out = sess.run(model.G, feed_dict = {model.z:z_test, model.alpha:[alpha], \n",
    "                                                           model.batch_size: 5, model.size_list: m_size_list})\n",
    "                    \n",
    "                    G_out = (G_out + 1.0)/2\n",
    "                    \n",
    "                    f1, ax1 = plt.subplots(1,5, figsize=(12,12))\n",
    "                    for k in range(5):\n",
    "                        img_RGB = cv2.cvtColor(x_in_resize[k,:,:,:], cv2.COLOR_BGR2RGB)\n",
    "                        ax1[k].imshow(img_RGB)\n",
    "                        ax1[k].axis('off')\n",
    "                        ax1[k].set_title('Sample '+str(k))\n",
    "\n",
    "                    f2, ax2 = plt.subplots(1,5, figsize=(12,12))\n",
    "                    for k in range(5):\n",
    "                        img_RGB = cv2.cvtColor(G_out[k,:,:,:], cv2.COLOR_BGR2RGB) \n",
    "                        ax2[k].imshow(img_RGB)\n",
    "                        ax2[k].axis('off')\n",
    "                        ax2[k].set_title('Image '+str(k))\n",
    "\n",
    "                    plt.show()\n",
    "\n",
    "                    count_step = 0\n",
    "\n",
    "                    # Learning rate decay\n",
    "                    learning_rate_g *= lr_g_decay_rate\n",
    "                    learning_rate_d *= lr_d_decay_rate\n",
    "                \n",
    "                count_step+=1\n",
    "                \n",
    "#             learning_rate_g += 2e-5\n",
    "        \n",
    "        # Test Result of Each Level\n",
    "        num_test = 10\n",
    "\n",
    "        img = np.zeros([size_list[m][-1] * num_test, size_list[m][-1] * num_test, 3])\n",
    "\n",
    "        z_result = np.random.uniform(-1, 1, size=(num_test**2, n_latent))\n",
    "        G_result = sess.run(model.G, feed_dict = {model.z: z_result, model.alpha: [1.0], \n",
    "                                                  model.batch_size: num_test**2, model.size_list: m_size_list})\n",
    "        G_result = (G_result + 1.0)/2\n",
    "        \n",
    "        for i in range(num_test**2):\n",
    "            row_num = int(i/num_test)\n",
    "            col_num = int(i%num_test)\n",
    "\n",
    "            img[row_num * size_list[m][-1] : (row_num + 1) * size_list[m][-1], \n",
    "                (col_num) * size_list[m][-1] : (col_num + 1) * size_list[m][-1]] = G_result[i,:,:,:]\n",
    "        \n",
    "        img_RGB = cv2.cvtColor(np.float32(img), cv2.COLOR_BGR2RGB) \n",
    "        \n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.imshow(img_RGB)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        #Save Model\n",
    "        os.mkdir(save_path + \"/\" + str(size_list[m][-1]))\n",
    "        Saver.save(sess, save_path + \"/\" + str(size_list[m][-1]) + \"/model\")\n",
    "        print(\"Model is saved in {}\".format(save_path + \"/\" + str(size_list[m][-1]) + \"/model\"))\n",
    "\n",
    "        model_old = model\n",
    "        \n",
    "        print('\\n------------------ Level {} is DONE!! ------------------\\n'.format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)\n",
    "print(size_up_idx)\n",
    "print(m_size_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Result of Each Level\n",
    "num_test = 10\n",
    "\n",
    "img = np.zeros([size_list[m] * num_test, size_list[m] * num_test, 3])\n",
    "\n",
    "z_result = np.random.uniform(-1, 1, size=(num_test**2, n_latent))\n",
    "G_result = sess.run(model.G, feed_dict = {model.z: z_result,  \n",
    "                                          model.alpha: [1.0], model.batch_size: num_test**2})\n",
    "G_result = (G_result + 1.0)/2\n",
    "\n",
    "for i in range(num_test**2):\n",
    "    row_num = int(i/num_test)\n",
    "    col_num = int(i%num_test)\n",
    "\n",
    "    img[row_num * size_list[m] : (row_num + 1) * size_list[m], \n",
    "        (col_num) * size_list[m] : (col_num + 1) * size_list[m]] = G_result[i,:,:,:]\n",
    "\n",
    "img_RGB = cv2.cvtColor(np.float32(img), cv2.COLOR_BGR2RGB) \n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img_RGB)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
