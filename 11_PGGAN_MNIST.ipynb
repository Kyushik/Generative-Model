{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGGAN MNIST\n",
    "\n",
    "This notebook is for implementing `Progressive Growing Generative Adversarial Network(PGGAN)` from the paper [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/abs/1710.10196) with [Tensorflow](https://www.tensorflow.org). <br>\n",
    "[MNIST data](http://yann.lecun.com/exdb/mnist/) will be used.\n",
    "\n",
    "Reference: [hwalsuklee's Github](https://github.com/hwalsuklee/tensorflow-generative-model-collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Q\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'PGGAN_MNIST'\n",
    "\n",
    "img_size   = 28\n",
    "\n",
    "batch_size = 32\n",
    "num_epoch  = 15\n",
    "\n",
    "n_latent = 512\n",
    "\n",
    "beta1 = 0\n",
    "beta2 = 0.99\n",
    "\n",
    "learning_rate_g = 0.00025\n",
    "learning_rate_d = 0.00025\n",
    "\n",
    "show_result_epoch = 1\n",
    "\n",
    "size_list = [8, 16, 32]\n",
    "channel_list = [128, 256, 512]\n",
    "\n",
    "# size_list = [4, 8, 16, 32]\n",
    "# channel_list = [64, 128, 256, 512]\n",
    "\n",
    "# size_list = [4, 8, 16, 32]\n",
    "# channel_list = [32, 64, 128, 128]\n",
    "\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d-%H-%M-%S\")\n",
    "\n",
    "load_model = False\n",
    "train_model = True\n",
    "\n",
    "save_path = \"./saved_models/\" + date_time + \"_\" + algorithm\n",
    "load_path = \"./saved_models/20190809-11-04-47_PGGAN_MNIST/model/model\" \n",
    "\n",
    "# WGAN_GP Parateter\n",
    "n_critic = 1\n",
    "d_lambda = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST Dataset\n",
    "\n",
    "Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist.load_data(path='mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "y_train shape: (60000,)\n",
      "x_test shape: (10000, 28, 28)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "x_train = mnist[0][0]\n",
    "y_train = mnist[0][1]\n",
    "x_test  = mnist[1][0]\n",
    "y_test  = mnist[1][1]\n",
    "\n",
    "print('x_train shape: {}'.format(x_train.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))\n",
    "print('x_test shape: {}'.format(x_test.shape))\n",
    "print('y_test shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGGAN():\n",
    "    def __init__(self, stage):\n",
    "        self.stage = stage\n",
    "        self.channel_list = channel_list\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, size_list[stage], size_list[stage], 1], name='x_'+str(stage))\n",
    "        self.x_normalize = (tf.cast(self.x, tf.float32) - (1.0/2)) / (1.0/2)\n",
    "\n",
    "        self.z = tf.placeholder(tf.float32, shape=[None, n_latent], name='z_'+str(stage))\n",
    "        \n",
    "        self.alpha = tf.placeholder(tf.float32, shape=[1])\n",
    "        \n",
    "        self.is_training = tf.placeholder(tf.bool, name='is_training_'+str(stage))\n",
    "\n",
    "        self.d_loss, self.g_loss, self.G = self.GAN(self.x_normalize, self.z, self.is_training) \n",
    "\n",
    "        # optimization\n",
    "        self.trainable_vars = tf.trainable_variables()\n",
    "\n",
    "        self.trainable_vars_d = [var for var in self.trainable_vars if var.name.startswith('Discriminator' + str(self.stage))]\n",
    "        self.trainable_vars_g = [var for var in self.trainable_vars if var.name.startswith('Generator' + str(self.stage))]\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.train_step_d = tf.train.AdamOptimizer(learning_rate_d).minimize(self.d_loss, var_list=self.trainable_vars_d)\n",
    "            self.train_step_g = tf.train.AdamOptimizer(learning_rate_g).minimize(self.g_loss, var_list=self.trainable_vars_g)\n",
    "\n",
    "    def GAN(self, x, z, is_training):\n",
    "        # Generator\n",
    "        G = self.Generator(z, is_training)\n",
    "\n",
    "        # Discriminator\n",
    "        D_logit_real, D_out_real = self.Discriminator(x, is_training)\n",
    "        D_logit_fake, D_out_fake = self.Discriminator(G, is_training, reuse=True)\n",
    "\n",
    "        # get loss \n",
    "        ########################################### WGAN GP ###########################################\n",
    "        eps = tf.random_uniform(shape=tf.shape(x),minval=0.0, maxval=1.0)       \n",
    "        x_hat = (eps * x) + ((1-eps) * G)\n",
    "        D_hat, _ = self.Discriminator(x_hat, is_training, reuse=True)\n",
    "        grad = tf.gradients(D_hat, [x_hat])[0]\n",
    "        GP = d_lambda * tf.square(tf.norm(grad, ord=2) - 1)\n",
    "\n",
    "        d_loss = -tf.reduce_mean(D_logit_real) + tf.reduce_mean(D_logit_fake) + GP\n",
    "        g_loss = -tf.reduce_mean(D_logit_fake) \n",
    "        ###############################################################################################\n",
    "    \n",
    "        return d_loss, g_loss , G\n",
    "    \n",
    "    def deconv_block(self, x, num_channel, is_training, network_name):\n",
    "        h1 = tf.layers.conv2d(x, filters=num_channel, kernel_size=3, strides=1, padding='SAME', name=network_name+'_1')\n",
    "        h1 = tf.nn.leaky_relu(h1) \n",
    "        \n",
    "        h2 = tf.layers.conv2d(h1, filters=num_channel, kernel_size=3, strides=1, padding='SAME', name=network_name+'_2')\n",
    "        h2 = tf.nn.leaky_relu(h2) \n",
    "        \n",
    "        h3 = tf.layers.conv2d(h2, filters=num_channel, kernel_size=3, strides=1, padding='SAME', name=network_name+'_3')\n",
    "        h3 = tf.nn.leaky_relu(h3) \n",
    "               \n",
    "        return h3\n",
    "\n",
    "    def Generator(self, x, is_training, reuse=False):\n",
    "        with tf.variable_scope('Generator' + str(self.stage), reuse=reuse):       \n",
    "            # Project and Reshape \n",
    "            x_project = tf.layers.dense(x, 1*1*512)\n",
    "            x_reshape = tf.reshape(x_project, (-1, 1, 1, 512))\n",
    "            \n",
    "            h1 = tf.layers.conv2d_transpose(x_reshape,filters=channel_list[-1], kernel_size=4, strides=4, \n",
    "                                            activation=tf.nn.leaky_relu, padding='SAME')\n",
    "            h2 = tf.layers.conv2d(h1,filters=channel_list[-1], kernel_size=3, strides=1, padding='SAME')\n",
    "            h2 = tf.nn.leaky_relu(h2)  \n",
    "        \n",
    "            in_block = h2\n",
    "            out_block = h2\n",
    "            upsample = h2\n",
    "\n",
    "            for i in range(self.stage):\n",
    "                upsample = tf.image.resize_nearest_neighbor(in_block, (size_list[i+1], size_list[i+1]))\n",
    "                out_block = self.deconv_block(upsample, channel_list[-2-i], is_training, 'block'+str(i))\n",
    "                in_block = out_block\n",
    "\n",
    "            # Output layer           \n",
    "            RGB1 = tf.layers.conv2d(upsample, filters=1, kernel_size=1, strides=1, padding='SAME', name='RGB1_'+str(self.stage)) \n",
    "            RGB2 = tf.layers.conv2d(out_block, filters=1, kernel_size=1, strides=1, padding='SAME', name='RGB2_'+str(self.stage)) \n",
    "            \n",
    "            logits = (1-self.alpha)*RGB1 + self.alpha*RGB2\n",
    "            \n",
    "            output = tf.tanh(logits)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def conv_block(self, x, num_channel, network_name):       \n",
    "        h1 = tf.layers.conv2d(x, filters=num_channel, kernel_size=3, strides=1, \n",
    "                              activation=tf.nn.leaky_relu, padding='SAME', name=network_name+'_1')\n",
    "        h2 = tf.layers.conv2d(h1, filters=num_channel, kernel_size=3, strides=1, \n",
    "                              activation=tf.nn.leaky_relu, padding='SAME', name=network_name+'_2')\n",
    "        h3 = tf.layers.conv2d(h2, filters=num_channel, kernel_size=3, strides=1, \n",
    "                              activation=tf.nn.leaky_relu, padding='SAME', name=network_name+'_3')\n",
    "        \n",
    "        return h3\n",
    "\n",
    "    def Discriminator(self, x, is_training, reuse=False):\n",
    "        with tf.variable_scope('Discriminator' + str(self.stage), reuse=reuse):\n",
    "            \n",
    "            h1 = tf.layers.conv2d(x, filters=channel_list[-self.stage-1], kernel_size=1, strides=1, activation=tf.nn.leaky_relu, \n",
    "                      padding='SAME', name='h1_'+str(self.stage))\n",
    "                \n",
    "            in_block = h1\n",
    "            out_block = h1\n",
    "            \n",
    "            for i in range(self.stage):\n",
    "                out_block = self.conv_block(in_block, channel_list[-self.stage+i], 'block'+str(self.stage-1-i))\n",
    "                \n",
    "                if i == 0:\n",
    "                    out_downsample = tf.layers.average_pooling2d(out_block, 2, 2)\n",
    "                    in_downsample = tf.layers.average_pooling2d(in_block, 2, 2)\n",
    "                    in_feature = tf.layers.conv2d(in_downsample, filters=out_downsample.get_shape()[3], \n",
    "                                                  kernel_size=1, strides=1, padding='SAME', name='feature_'+str(self.stage))\n",
    "                    out_block = (1-self.alpha)*in_feature + self.alpha*out_downsample\n",
    "                else:\n",
    "                    out_block = tf.layers.average_pooling2d(out_block, 2, 2)\n",
    "                    \n",
    "                in_block = out_block           \n",
    "            \n",
    "            # Output layer\n",
    "            h2 = tf.layers.conv2d(out_block, filters=512, kernel_size=3, strides=1, \n",
    "                                  activation=tf.nn.leaky_relu, padding='SAME', name='h2')\n",
    "            h3 = tf.layers.conv2d(h2, filters=512, kernel_size=3, strides=1, \n",
    "                                  activation=tf.nn.leaky_relu, padding='SAME', name='h3')\n",
    "            h4 = tf.layers.conv2d(h3, filters=512, kernel_size=4, strides=4, \n",
    "                                  activation=tf.nn.leaky_relu, padding='SAME', name='h4')\n",
    "            \n",
    "            # Output layer\n",
    "            flatten = tf.reshape(h4, (-1, h4.get_shape()[3]))\n",
    "\n",
    "            logit  = tf.layers.dense(flatten, 1, name='logit_'+str(self.stage))\n",
    "            output = tf.sigmoid(logit)  \n",
    "\n",
    "        return logit, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 8 and 4 for 'mul_1' (op: 'Mul') with input shapes: [?,8,8,1], [?,4,4,1].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1628\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1629\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 8 and 4 for 'mul_1' (op: 'Mul') with input shapes: [?,8,8,1], [?,4,4,1].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-202b8c89fb31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPGGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-93aca935a18f>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, stage)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'is_training_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_normalize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-93aca935a18f>\u001b[0m in \u001b[0;36mGAN\u001b[1;34m(self, x, z, is_training)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m########################################### WGAN GP ###########################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mx_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mD_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    864\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1129\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Case: Dense * Sparse.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   5356\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5357\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 5358\u001b[1;33m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[0;32m   5359\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5360\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m                 instructions)\n\u001b[1;32m--> 488\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[0;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3272\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3273\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3274\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3276\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1790\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1791\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1792\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 8 and 4 for 'mul_1' (op: 'Mul') with input shapes: [?,8,8,1], [?,4,4,1]."
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "\n",
    "for i in range(len(size_list)):\n",
    "    model_list.append(PGGAN(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Saver = tf.train.Saver()\n",
    "\n",
    "if load_model == True:\n",
    "    Saver.restore(sess, load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    # Training\n",
    "    data_x = x_train\n",
    "    len_data = x_train.shape[0]\n",
    "\n",
    "    for m in range(len(size_list)):\n",
    "        \n",
    "        model = model_list[m]\n",
    "        alpha = 0\n",
    "        \n",
    "        if m != 0:\n",
    "            model_var_d = model.trainable_vars_d\n",
    "            model_var_g = model.trainable_vars_g\n",
    "            \n",
    "            model_old_var_d = model_old.trainable_vars_d\n",
    "            model_old_var_g = model_old.trainable_vars_g\n",
    "            \n",
    "            for var_d in model_var_d:\n",
    "                var_d_split = var_d.name.split('/')\n",
    "                for var_old_d in model_old_var_d:\n",
    "                    var_old_d_split = var_old_d.name.split('/')\n",
    "                    \n",
    "                    if var_d_split[1] == var_old_d_split[1] and var_d_split[2] == var_old_d_split[2]:\n",
    "                        sess.run(var_old_d.assign(var_d))\n",
    "\n",
    "            for var_g in model_var_g:\n",
    "                var_g_split = var_g.name.split('/')\n",
    "                for var_old_g in model_old_var_g:\n",
    "                    var_old_g_split = var_old_g.name.split('/')\n",
    "                    \n",
    "                    if var_g_split[1] == var_old_g_split[1] and var_g_split[2] == var_old_g_split[2]:\n",
    "                        sess.run(var_old_g.assign(var_g))\n",
    "            \n",
    "            print('------------------- Var Assign is Finished! -------------------')\n",
    "        for i in range(num_epoch):\n",
    "            # Shuffle the data \n",
    "            np.random.shuffle(data_x)\n",
    "            \n",
    "            alpha = (2*i)/num_epoch\n",
    "\n",
    "            if alpha > 1:\n",
    "                alpha = 1\n",
    "                    \n",
    "            # Making mini-batch\n",
    "            for j in range(0, len_data, batch_size):\n",
    "                if j + batch_size < len_data:\n",
    "                    x_in = data_x[j : j + batch_size, :]\n",
    "                else:\n",
    "                    x_in = data_x[j : len_data, :]\n",
    "\n",
    "                x_in = x_in.reshape((-1, img_size, img_size, 1))\n",
    "                \n",
    "                x_in_resize = np.zeros([x_in.shape[0], size_list[m], size_list[m], 1])\n",
    "                \n",
    "                for k in range(x_in.shape[0]):\n",
    "                    x_in_resize[k,:,:,:] = resize(x_in[k,:,:,:], (size_list[m], size_list[m]))\n",
    "                    \n",
    "                sampled_z = np.random.uniform(-1, 1, size=(x_in_resize.shape[0] , n_latent))              \n",
    "                \n",
    "                # Run Optimizer!\n",
    "                _, loss_d = sess.run([model.train_step_d, model.d_loss], feed_dict = {model.x: x_in_resize, \n",
    "                                                                                      model.z: sampled_z,\n",
    "                                                                                      model.is_training: True,\n",
    "                                                                                      model.alpha: [alpha]})\n",
    "                _, loss_g = sess.run([model.train_step_g, model.g_loss], feed_dict = {model.x: x_in_resize, \n",
    "                                                                                      model.z: sampled_z, \n",
    "                                                                                      model.is_training: True,\n",
    "                                                                                      model.alpha: [alpha]})\n",
    "\n",
    "                print(\"Batch: {} / {}\".format(j, len_data), end=\"\\r\")\n",
    "\n",
    "            # Print Progess\n",
    "            print(\"Epoch: {} / G Loss: {:.5f} / D Loss: {:.5f}\".format((i+1), loss_g, loss_d))\n",
    "\n",
    "            # Show test images \n",
    "            z_test = np.random.uniform(-1, 1, size=(5, n_latent))\n",
    "            G_out = sess.run(model.G, feed_dict = {model.z: z_test, model.is_training: False, model.alpha: [alpha]})\n",
    "\n",
    "            if i == 0 or (i+1) % show_result_epoch == 0:\n",
    "                f1, ax1 = plt.subplots(1,5)\n",
    "                for j in range(5):\n",
    "                    ax1[j].imshow(x_in_resize[j,:,:,0], cmap = 'gray')\n",
    "                    ax1[j].axis('off')\n",
    "                    ax1[j].set_title('Sample '+str(j))\n",
    "                    \n",
    "            if i == 0 or (i+1) % show_result_epoch == 0:\n",
    "                f2, ax2 = plt.subplots(1,5)\n",
    "                for j in range(5):\n",
    "                    ax2[j].imshow(G_out[j,:,:,0], cmap = 'gray')\n",
    "                    ax2[j].axis('off')\n",
    "                    ax2[j].set_title('Image '+str(j))\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "        model_old = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(var_old_d)\n",
    "print(var_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 10\n",
    "\n",
    "img = np.zeros([size_list[-1] * num_test, size_list[-1] * num_test])\n",
    "\n",
    "z_result = np.random.uniform(-1, 1, size=(num_test**2, n_latent))\n",
    "G_result = sess.run(model.G, feed_dict = {z: z_result})\n",
    "\n",
    "for i in range(num_test**2):\n",
    "    row_num = int(i/num_test)\n",
    "    col_num = int(i%num_test)\n",
    "    \n",
    "    img[row_num * img_size : (row_num + 1) * img_size, (col_num) * img_size : (col_num + 1) * img_size] = G_result[i,:,:,0]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(save_path)\n",
    "\n",
    "Saver.save(sess, save_path + \"/model/model\")\n",
    "print(\"Model is saved in {}\".format(save_path + \"/model/model\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
