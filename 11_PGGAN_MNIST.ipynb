{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGGAN MNIST\n",
    "\n",
    "This notebook is for implementing `Progressive Growing Generative Adversarial Network(PGGAN)` from the paper [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/abs/1710.10196) with [Tensorflow](https://www.tensorflow.org). <br>\n",
    "[MNIST data](http://yann.lecun.com/exdb/mnist/) will be used.\n",
    "\n",
    "Reference: [hwalsuklee's Github](https://github.com/hwalsuklee/tensorflow-generative-model-collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'PGGAN_MNIST'\n",
    "\n",
    "img_size   = 28\n",
    "\n",
    "batch_size = 64\n",
    "num_epoch  = 15\n",
    "\n",
    "n_latent = 512\n",
    "\n",
    "beta1 = 0\n",
    "beta2 = 0.99\n",
    "\n",
    "learning_rate_g = 0.00001\n",
    "learning_rate_d = 0.00001\n",
    "\n",
    "show_result_epoch = 1\n",
    "\n",
    "# size_list = [8, 16, 32]\n",
    "# channel_list = [128, 256, 512]\n",
    "\n",
    "size_list = [4, 8, 16, 32]\n",
    "channel_list = [64, 128, 256, 512]\n",
    "\n",
    "# size_list = [4, 8, 16, 32]\n",
    "# channel_list = [32, 64, 128, 128]\n",
    "\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d-%H-%M-%S\")\n",
    "\n",
    "load_model = False\n",
    "train_model = True\n",
    "\n",
    "save_path = \"./saved_models/\" + date_time + \"_\" + algorithm\n",
    "load_path = \"./saved_models/20190809-11-04-47_PGGAN_MNIST/model/model\" \n",
    "\n",
    "# WGAN_GP Parateter\n",
    "n_critic = 1\n",
    "d_lambda = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST Dataset\n",
    "\n",
    "Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist.load_data(path='mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = mnist[0][0]\n",
    "y_train = mnist[0][1]\n",
    "x_test  = mnist[1][0]\n",
    "y_test  = mnist[1][1]\n",
    "\n",
    "print('x_train shape: {}'.format(x_train.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))\n",
    "print('x_test shape: {}'.format(x_test.shape))\n",
    "print('y_test shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGGAN():\n",
    "    def __init__(self, stage):\n",
    "        self.stage = stage\n",
    "        self.channel_list = channel_list\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, size_list[stage], size_list[stage], 1], name='x_'+str(stage))\n",
    "        self.x_normalize = (tf.cast(self.x, tf.float32) - (1.0/2)) / (1.0/2)\n",
    "\n",
    "        self.z = tf.placeholder(tf.float32, shape=[None, n_latent], name='z_'+str(stage))\n",
    "        \n",
    "        self.alpha = tf.placeholder(tf.float32, shape=[1])\n",
    "        \n",
    "        self.is_training = tf.placeholder(tf.bool, name='is_training_'+str(stage))\n",
    "\n",
    "        self.d_loss, self.g_loss, self.G = self.GAN(self.x_normalize, self.z, self.is_training) \n",
    "\n",
    "        # optimization\n",
    "        self.trainable_vars = tf.trainable_variables()\n",
    "\n",
    "        self.trainable_vars_d = [var for var in self.trainable_vars if var.name.startswith('Discriminator' + str(self.stage))]\n",
    "        self.trainable_vars_g = [var for var in self.trainable_vars if var.name.startswith('Generator' + str(self.stage))]\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.train_step_d = tf.train.AdamOptimizer(learning_rate_d).minimize(self.d_loss, var_list=self.trainable_vars_d)\n",
    "            self.train_step_g = tf.train.AdamOptimizer(learning_rate_g).minimize(self.g_loss, var_list=self.trainable_vars_g)\n",
    "\n",
    "    def GAN(self, x, z, is_training):\n",
    "        # Generator\n",
    "        G, mini_std = self.Generator(z, is_training)\n",
    "\n",
    "        # Discriminator\n",
    "        D_logit_real, D_out_real = self.Discriminator(x, mini_std, is_training)\n",
    "        D_logit_fake, D_out_fake = self.Discriminator(G, mini_std, is_training, reuse=True)\n",
    "\n",
    "        # get loss \n",
    "        ########################################### WGAN GP ###########################################\n",
    "        eps = tf.random_uniform(shape=tf.shape(x),minval=0.0, maxval=1.0)       \n",
    "        x_hat = (eps * x) + ((1-eps) * G)\n",
    "        D_hat, _ = self.Discriminator(x_hat, mini_std, is_training, reuse=True)\n",
    "        grad = tf.gradients(D_hat, [x_hat])[0]\n",
    "        GP = d_lambda * tf.square(tf.norm(grad, ord=2) - 1)\n",
    "\n",
    "        d_loss = -tf.reduce_mean(D_logit_real) + tf.reduce_mean(D_logit_fake) + GP\n",
    "        g_loss = -tf.reduce_mean(D_logit_fake) \n",
    "        ###############################################################################################\n",
    "    \n",
    "        return d_loss, g_loss , G\n",
    "    \n",
    "    # Pixel Normalization\n",
    "    def pixel_norm(self, x, epsilon=1e-8):\n",
    "        return x * tf.rsqrt(tf.reduce_mean(tf.square(x), axis=-1, keepdims=True) + epsilon)\n",
    "    \n",
    "    # Minibatch Standard Deviation\n",
    "    def minibatch_std(self, x):\n",
    "        # Compute Standard Deviation over minibatch\n",
    "        _, batch_std = tf.nn.moments(x, axes=[0], keep_dims=True)\n",
    "        \n",
    "        # Average all features\n",
    "        feature_avg = tf.reduce_mean(batch_std, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Replicate the value to concatenate it over the minibatch\n",
    "        out = tf.tile(feature_avg, multiples=[batch_size, 1, 1, 1])\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def conv_block(self, x, num_channel, network_name):\n",
    "        h1 = tf.layers.conv2d(x, filters=num_channel, kernel_size=3, strides=1, padding='SAME', name=network_name+'_1')\n",
    "        h1 = self.pixel_norm(tf.nn.leaky_relu(h1)) \n",
    "        \n",
    "        h2 = tf.layers.conv2d(h1, filters=num_channel, kernel_size=3, strides=1, padding='SAME', name=network_name+'_2')\n",
    "        h2 = self.pixel_norm(tf.nn.leaky_relu(h2)) \n",
    "        \n",
    "        h3 = tf.layers.conv2d(h2, filters=num_channel, kernel_size=3, strides=1, padding='SAME', name=network_name+'_3')\n",
    "        h3 = self.pixel_norm(tf.nn.leaky_relu(h3)) \n",
    "               \n",
    "        return h3\n",
    "\n",
    "    def Generator(self, x, is_training, reuse=False):\n",
    "        with tf.variable_scope('Generator' + str(self.stage), reuse=reuse):       \n",
    "            # Project and Reshape \n",
    "            z_size = int(size_list[0]/4)\n",
    "            \n",
    "            x_project = tf.layers.dense(x, z_size*z_size*512)\n",
    "            x_reshape = tf.reshape(x_project, (-1, z_size, z_size, 512))\n",
    "            \n",
    "            h1 = tf.layers.conv2d_transpose(x_reshape,filters=channel_list[-1], kernel_size=4, strides=4, padding='SAME')\n",
    "            h1 = self.pixel_norm(tf.nn.leaky_relu(h1)) \n",
    "            \n",
    "            h2 = tf.layers.conv2d(h1,filters=channel_list[-1], kernel_size=3, strides=1, padding='SAME')\n",
    "            h2 = self.pixel_norm(tf.nn.leaky_relu(h2))  \n",
    "\n",
    "            h3 = tf.layers.conv2d(h2,filters=channel_list[-1], kernel_size=3, strides=1, padding='SAME')\n",
    "            h3 = self.pixel_norm(tf.nn.leaky_relu(h3))  \n",
    "            \n",
    "            mini_std = self.minibatch_std(h3)\n",
    "        \n",
    "            in_block = h3\n",
    "            out_block = h3\n",
    "            upsample = h3\n",
    "\n",
    "            for i in range(self.stage):\n",
    "                upsample = tf.image.resize_nearest_neighbor(in_block, (size_list[i+1], size_list[i+1]))\n",
    "                out_block = self.conv_block(upsample, channel_list[-2-i], 'block'+str(i))\n",
    "                in_block = out_block\n",
    "\n",
    "            # Output layer           \n",
    "            RGB1 = tf.layers.conv2d(upsample, filters=1, kernel_size=1, strides=1, padding='SAME', name='RGB1_'+str(self.stage)) \n",
    "            RGB2 = tf.layers.conv2d(out_block, filters=1, kernel_size=1, strides=1, padding='SAME', name='RGB2_'+str(self.stage)) \n",
    "            \n",
    "            logits = (1-self.alpha)*RGB1 + self.alpha*RGB2\n",
    "            \n",
    "            output = tf.tanh(logits)\n",
    "        \n",
    "        return output, mini_std      \n",
    "\n",
    "\n",
    "    def Discriminator(self, x, mini_std, is_training, reuse=False):\n",
    "        with tf.variable_scope('Discriminator' + str(self.stage), reuse=reuse):\n",
    "            \n",
    "            h1 = tf.layers.conv2d(x, filters=channel_list[-self.stage-1], kernel_size=1, strides=1, activation=tf.nn.leaky_relu, \n",
    "                      padding='SAME', name='h1_'+str(self.stage))\n",
    "                \n",
    "            in_block = h1\n",
    "            out_block = h1\n",
    "            \n",
    "            for i in range(self.stage):\n",
    "                out_block = self.conv_block(in_block, channel_list[-self.stage+i], 'block'+str(self.stage-1-i))\n",
    "                \n",
    "                if i == 0:\n",
    "                    out_downsample = tf.layers.average_pooling2d(out_block, 2, 2)\n",
    "                    in_downsample = tf.layers.average_pooling2d(in_block, 2, 2)\n",
    "                    in_feature = tf.layers.conv2d(in_downsample, filters=out_downsample.get_shape()[3], \n",
    "                                                  kernel_size=1, strides=1, padding='SAME', name='feature_'+str(self.stage))\n",
    "                    out_block = (1-self.alpha)*in_feature + self.alpha*out_downsample\n",
    "                else:\n",
    "                    out_block = tf.layers.average_pooling2d(out_block, 2, 2)\n",
    "                    \n",
    "                in_block = out_block           \n",
    "            \n",
    "            # Output layer\n",
    "            out_add_std = tf.concat([out_block, mini_std], axis=-1)\n",
    "            \n",
    "            h2 = tf.layers.conv2d(out_add_std, filters=256, kernel_size=3, strides=1, \n",
    "                                  activation=tf.nn.leaky_relu, padding='SAME', name='h2')\n",
    "            h3 = tf.layers.conv2d(h2, filters=256, kernel_size=4, strides=4, \n",
    "                                  activation=tf.nn.leaky_relu, padding='SAME', name='h4')\n",
    "            \n",
    "            # Output layer\n",
    "            flatten = tf.reshape(h3, (-1, h3.get_shape()[3]))\n",
    "\n",
    "            logit  = tf.layers.dense(flatten, 1, name='logit_'+str(self.stage))\n",
    "            output = tf.sigmoid(logit)  \n",
    "\n",
    "        return logit, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "\n",
    "for i in range(len(size_list)):\n",
    "    model_list.append(PGGAN(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Saver = tf.train.Saver()\n",
    "\n",
    "if load_model == True:\n",
    "    Saver.restore(sess, load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    # Training\n",
    "    data_x = x_train\n",
    "    len_data = x_train.shape[0]\n",
    "\n",
    "    for m in range(len(size_list)):\n",
    "        \n",
    "        model = model_list[m]\n",
    "        alpha = 0\n",
    "        \n",
    "        if m != 0:\n",
    "            model_var_d = model.trainable_vars_d\n",
    "            model_var_g = model.trainable_vars_g\n",
    "            \n",
    "            model_old_var_d = model_old.trainable_vars_d\n",
    "            model_old_var_g = model_old.trainable_vars_g\n",
    "            \n",
    "            for var_d in model_var_d:\n",
    "                var_d_split = var_d.name.split('/')\n",
    "                for var_old_d in model_old_var_d:\n",
    "                    var_old_d_split = var_old_d.name.split('/')\n",
    "                    \n",
    "                    if var_d_split[1] == var_old_d_split[1] and var_d_split[2] == var_old_d_split[2]:\n",
    "                        sess.run(var_old_d.assign(var_d))\n",
    "\n",
    "            for var_g in model_var_g:\n",
    "                var_g_split = var_g.name.split('/')\n",
    "                for var_old_g in model_old_var_g:\n",
    "                    var_old_g_split = var_old_g.name.split('/')\n",
    "                    \n",
    "                    if var_g_split[1] == var_old_g_split[1] and var_g_split[2] == var_old_g_split[2]:\n",
    "                        sess.run(var_old_g.assign(var_g))\n",
    "            \n",
    "            print('------------------- Var Assign is Finished! -------------------')\n",
    "        for i in range(num_epoch):\n",
    "            # Shuffle the data \n",
    "            np.random.shuffle(data_x)\n",
    "            \n",
    "            alpha = (2*i)/num_epoch\n",
    "\n",
    "            if alpha > 1:\n",
    "                alpha = 1\n",
    "                    \n",
    "            # Making mini-batch\n",
    "            for j in range(0, len_data, batch_size):\n",
    "                if j + batch_size < len_data:\n",
    "                    x_in = data_x[j : j + batch_size, :]\n",
    "                else:\n",
    "                    x_in = data_x[j : len_data, :]\n",
    "\n",
    "                x_in = x_in.reshape((-1, img_size, img_size, 1))\n",
    "                \n",
    "                x_in_resize = np.zeros([x_in.shape[0], size_list[m], size_list[m], 1])\n",
    "                \n",
    "                for k in range(x_in.shape[0]):\n",
    "                    x_in_resize[k,:,:,:] = resize(x_in[k,:,:,:], (size_list[m], size_list[m]))\n",
    "                    \n",
    "                sampled_z = np.random.uniform(-1, 1, size=(x_in_resize.shape[0] , n_latent))              \n",
    "                \n",
    "                # Run Optimizer!\n",
    "                _, loss_d = sess.run([model.train_step_d, model.d_loss], feed_dict = {model.x: x_in_resize, \n",
    "                                                                                      model.z: sampled_z,\n",
    "                                                                                      model.is_training: True,\n",
    "                                                                                      model.alpha: [alpha]})\n",
    "                _, loss_g = sess.run([model.train_step_g, model.g_loss], feed_dict = {model.x: x_in_resize, \n",
    "                                                                                      model.z: sampled_z, \n",
    "                                                                                      model.is_training: True,\n",
    "                                                                                      model.alpha: [alpha]})\n",
    "\n",
    "                print(\"Batch: {} / {}\".format(j, len_data), end=\"\\r\")\n",
    "\n",
    "            # Print Progess\n",
    "            print(\"Epoch: {} / G Loss: {:.5f} / D Loss: {:.5f}\".format((i+1), loss_g, loss_d))\n",
    "\n",
    "            # Show test images \n",
    "            z_test = np.random.uniform(-1, 1, size=(5, n_latent))\n",
    "            G_out = sess.run(model.G, feed_dict = {model.z: z_test, model.is_training: False, model.alpha: [alpha]})\n",
    "\n",
    "            if i == 0 or (i+1) % show_result_epoch == 0:\n",
    "                f1, ax1 = plt.subplots(1,5)\n",
    "                for j in range(5):\n",
    "                    ax1[j].imshow(x_in_resize[j,:,:,0], cmap = 'gray')\n",
    "                    ax1[j].axis('off')\n",
    "                    ax1[j].set_title('Sample '+str(j))\n",
    "                    \n",
    "            if i == 0 or (i+1) % show_result_epoch == 0:\n",
    "                f2, ax2 = plt.subplots(1,5)\n",
    "                for j in range(5):\n",
    "                    ax2[j].imshow(G_out[j,:,:,0], cmap = 'gray')\n",
    "                    ax2[j].axis('off')\n",
    "                    ax2[j].set_title('Image '+str(j))\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "        model_old = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(var_old_d)\n",
    "print(var_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 10\n",
    "\n",
    "img = np.zeros([size_list[-1] * num_test, size_list[-1] * num_test])\n",
    "\n",
    "z_result = np.random.uniform(-1, 1, size=(num_test**2, n_latent))\n",
    "G_result = sess.run(model.G, feed_dict = {z: z_result})\n",
    "\n",
    "for i in range(num_test**2):\n",
    "    row_num = int(i/num_test)\n",
    "    col_num = int(i%num_test)\n",
    "    \n",
    "    img[row_num * img_size : (row_num + 1) * img_size, (col_num) * img_size : (col_num + 1) * img_size] = G_result[i,:,:,0]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(save_path)\n",
    "\n",
    "Saver.save(sess, save_path + \"/model/model\")\n",
    "print(\"Model is saved in {}\".format(save_path + \"/model/model\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
